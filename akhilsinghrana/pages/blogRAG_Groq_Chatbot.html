<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Building a Free, Serverless AI Chatbot in 2024: RAG, Llama 3.1, and Open-Source Magic</title>
    
    <!-- Meta description -->
    <meta name="description" content="Learn how to create a powerful, free, and serverless AI chatbot using RAG, Llama 3.1, Groq, Langchain, and Hugging Face in 2024. Perfect for websites and applications.">
    
    <!-- Open Graph meta tags for social sharing -->
    <meta property="og:title" content="Free Serverless AI Chatbot: RAG, Llama 3.1, and Open-Source in 2024">
    <meta property="og:description" content="Discover how to build a sophisticated, cost-free chatbot using RAG, Llama 3.1, Groq, and Hugging Face APIs. No server required!">
    <meta property="og:image" content="https://akhilsinghrana.com/#blog">
    <meta property="og:url" content="https://akhilsinghrana.com/#blog">
    <meta property="og:type" content="article">
    
    <!-- Twitter Card meta tags -->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="Free Serverless AI Chatbot: RAG, Llama 3.1, and Open-Source in 2024">
    <meta name="twitter:description" content="Build a powerful, free chatbot using RAG, Llama 3.1, Groq, and Hugging Face APIs. No server needed!">
    <meta name="twitter:image" content="https://akhilsinghrana.com/#blog">
    
    <!-- Structured data for rich snippets -->
    <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "TechArticle",
      "headline": "Building a Free, Serverless AI Chatbot in 2024: RAG, Llama 3.1, and Open-Source Magic",
      "description": "Learn how to create a powerful, free, and serverless AI chatbot using RAG, Llama 3.1, Groq, Langchain, and Hugging Face in 2024. Perfect for websites and applications.",
      "image": "https://akhilsinghrana.com/#blog",
      "author": {
        "@type": "Person",
        "name": "Akhil Singh Rana"
      },
      "datePublished": "2024-07-30",
      "publisher": {
        "@type": "Person",
        "name": "Akhil Singh Rana",
        "url": "https://akhilsinghrana.com"
      }
    }
    </script>
</head>
<body>
<article>
    <h1>Building a Free, Serverless AI Chatbot in 2024: RAG, Llama 3.1, and Open-Source Magic</h1>
    
    <p class="lead">In 2024, AI chatbots have become indispensable for websites and applications. This guide will walk you through creating a sophisticated, completely free, and serverless chatbot using cutting-edge technologies and the powerful Retrieval-Augmented Generation (RAG) approach.</p>
    
    <h2>Key Highlights</h2>
    <ul class="list-group">
        <li class="list-group-item">100% free and open-source implementation</li>
        <li class="list-group-item">Serverless architecture using Hugging Face and Groq APIs</li>
        <li class="list-group-item">Utilization of Meta's latest Llama 3.1 model via Groq</li>
        <li class="list-group-item">Advanced RAG (Retrieval-Augmented Generation) approach</li>
        <li class="list-group-item">Integration of Hugging Face embeddings and Chroma vector database</li>
        <li class="list-group-item">Dynamic web search capabilities with Tavily Search</li>
        <li class="list-group-item">Efficient document processing with HTMLHeaderTextSplitter</li>
    </ul>
    <br>
    <h2>Understanding Our Enhanced RAG Approach</h2>
    
    <div class="row">
        <div class="image-placeholder col-md-3">
            <img src="/static/Extra/imgs/RAGPipeline.png" alt="RAG flow">
        </div>
        <div class="col-md-9">
        <h3>The RAG Pipeline: A Closer Look</h3>

            <p>Our implementation takes the RAG approach to the next level:</p>

            <ol>
                <li><strong>Efficient Document Processing:</strong> We use HTMLHeaderTextSplitter to intelligently chunk HTML documents based on headers, ensuring context-aware text splitting.</li>
                
                <li><strong>Advanced Retrieval:</strong> The system retrieves relevant documents from the Chroma vector database using Hugging Face embeddings.</li>
                
                <li><strong>Intelligent Document Grading:</strong> Retrieved documents are evaluated for relevance using the Llama 3.1 model, determining if web search is necessary.</li>
                
                <li><strong>Dynamic Web Search:</strong> If needed, Tavily Search API is used to fetch up-to-date information from the web.</li>
                
                <li><strong>Context-Aware Generation:</strong> Using Groq's API to access Llama 3.1, the chatbot generates comprehensive answers based on retrieved documents and web search results.</li>
            </ol>

            <p>This enhanced pipeline ensures our chatbot provides accurate, relevant, and up-to-date information by seamlessly combining local knowledge with real-time web data.</p>
        </div>
    </div>
    <br>
    <h2>Our Free and Serverless Tech Stack</h2>
    
    <h3>1. LangChain: The Orchestrator</h3>
    <p>LangChain serves as the backbone, providing tools for document loading, text splitting, and workflow management.</p>
    
    <h3>2. Hugging Face Embeddings API (Free)</h3>
    <p>We leverage the Hugging Face Inference API for creating embeddings, using the powerful BAAI/bge-large-en-v1.5 model:</p>
    
    <pre><code>
from langchain_community.embeddings import HuggingFaceInferenceAPIEmbeddings

self.embeddings = HuggingFaceInferenceAPIEmbeddings(
    api_key=os.environ.get("HF_API_KEY"),
    model_name="BAAI/bge-large-en-v1.5",
    encode_kwargs={"normalize_embeddings": True}
)
    </code></pre>
    
    <h3>3. Chroma: Open-Source Vector Database</h3>
    <p>Chroma efficiently stores and retrieves document embeddings:</p>
    
    <pre><code>
from langchain_chroma import Chroma

vectorstore = Chroma.from_documents(
    documents=all_splits,
    embedding=self.embeddings,
    persist_directory=self.persistent_directory
)
    </code></pre>
    
    <h3>4. Groq API: Access to Llama 3.1 (Free with Rate Limits)</h3>
    <p>We use Groq's API to access Meta's latest Llama 3.1 model, which is free to use with rate limits:</p>
    
    <pre><code>
from langchain_groq import ChatGroq

self.llm = ChatGroq(
    model="llama-3.1-70b-versatile",
    temperature=0,
    max_tokens=None,
    timeout=None,
    max_retries=2,
)
    </code></pre>
    
    <h3>5. Tavily Search API (Free Tier Available)</h3>
    <p>To augment our chatbot with real-time web search capabilities:</p>
    
    <pre><code>
from langchain_community.tools.tavily_search import TavilySearchResults

self.web_search_tool = TavilySearchResults()
    </code></pre>
    
    <h2>Implementing Our Enhanced RAG Chatbot</h2>
    
    <p>Let's dive into the key components of our `RAGChat` class:</p>
    
    <h3>1. Efficient Document Processing</h3>
    <pre><code>
headers_to_split_on = [
    ("h1", "Header 1"),
    ("h2", "Header 2"),
    ("h3", "Header 3"),
    ("section", "Section"),
]
text_splitter = HTMLHeaderTextSplitter(headers_to_split_on=headers_to_split_on)

all_splits = []
for doc in raw_documents:
    splits = text_splitter.split_text(doc.page_content)
    all_splits.extend(splits)
    </code></pre>
    
    <h3>2. Intelligent Document Retrieval and Grading</h3>
    <pre><code>
def grade_documents(state):
    question = state["question"]
    documents = state["documents"]
    filtered_docs = []
    search = "No"
    for d in documents:
        score = self.retrieval_grader.invoke(
            {"question": question, "document": d.page_content}
        )
        grade = score["score"]
        if grade == "yes":
            filtered_docs.append(d)
        else:
            search = "Yes"
    return {
        "documents": filtered_docs,
        "question": question,
        "search": search,
        "steps": state["steps"] + ["grade_document_retrieval"],
    }
    </code></pre>
    
    <h3>3. Dynamic Web Search Integration</h3>
    <pre><code>
def web_search(state):
    question = state["question"]
    documents = state.get("documents", [])
    web_results = self.web_search_tool.invoke({"query": question})
    documents.extend(
        [
            Document(page_content=d["content"], metadata={"url": d["url"]})
            for d in web_results
        ]
    )
    return {
        "documents": documents,
        "question": question,
        "steps": state["steps"] + ["web_search"],
    }
    </code></pre>
    
    <h3>4. Context-Aware Answer Generation</h3>
    <pre><code>
def generate(state):
    question = state["question"]
    documents = state["documents"]
    generation = self.rag_chain.invoke(
        {"documents": documents, "question": question}
    )
    return {
        "documents": documents,
        "question": question,
        "generation": generation,
        "steps": state["steps"] + ["generate_answer"],
    }
    </code></pre>
    
    <h2>Orchestrating the Workflow with LangChain's StateGraph</h2>
    
    <p>We use LangChain's StateGraph to create a dynamic, decision-making workflow:</p>
    
    <pre><code>
workflow = StateGraph(GraphState)
workflow.add_node("retrieve", retrieve)
workflow.add_node("grade_documents", grade_documents)
workflow.add_node("generate", generate)
workflow.add_node("web_search", web_search)

workflow.set_entry_point("retrieve")
workflow.add_edge("retrieve", "grade_documents")
workflow.add_conditional_edges(
    "grade_documents",
    decide_to_generate,
    {
        "search": "web_search",
        "generate": "generate",
    },
)
workflow.add_edge("web_search", "generate")
workflow.add_edge("generate", END)
    </code></pre>
    
    <h2>Conclusion: The Future of Free, Serverless AI Chatbots</h2>
        
        <p>By harnessing the power of cutting-edge, free, and serverless tools like Hugging Face and Groq APIs, combined with open-source technologies such as LangChain and Chroma, we've created a sophisticated chatbot that can be easily integrated into any website or application without any hosting costs.</p>
        
        <p>This implementation offers a powerful combination of local knowledge base querying, intelligent document processing, and dynamic web search capabilities, ensuring that users receive accurate and up-to-date information. The use of the enhanced RAG approach, coupled with Meta's latest Llama 3.1 model via Groq, puts this chatbot at the forefront of AI technology in 2024.</p>
        
        <p>As we progress through 2024, such advanced, open-source, and serverless chatbot integrations will become increasingly crucial for developers and businesses looking to provide exceptional user experiences while maintaining full control over their AI implementations and data.</p>

        <p>For a deeper dive into this implementation, check out the complete code on my GitHub repository: <a href="https://github.com/AkhilSinghRana/akhilsinghrana.com/blob/homepage/akhilsinghrana/RAG_Chat.py" target="_blank">https://github.com/AkhilSinghRana/chatbot-integration-Blog</a>. Feel free to clone, fork, or star the repository if you find it useful!</p>

        <p>I'm always eager to discuss potential improvements or answer any questions about this approach. Reach out through the contact form on this website or connect with me on <a href="https://linkedin.com/in/akhilsinghrana" target="_blank">LinkedIn</a>. Your insights and questions are invaluable in advancing this technology and making it even more accessible to the community.</p>

        <p>Thank you for reading, and happy coding in the world of free, serverless AI!</p>
</article>

</body>
</html>